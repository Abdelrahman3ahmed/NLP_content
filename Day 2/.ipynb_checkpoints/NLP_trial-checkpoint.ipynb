{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2acea2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "635ee4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Honda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow \n",
    "import keras \n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer \n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1e854",
   "metadata": {},
   "source": [
    "# Read_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36653b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.read_csv('dataset.csv')\n",
    "Data_prep_2=Data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "491c6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.drop(columns='id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca51d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      0   @user when a father is dysfunctional and is s...\n",
       "1      0  @user @user thanks for #lyft credit i can't us...\n",
       "2      0                                bihday your majesty\n",
       "3      0  #model   i love u take with u all the time in ...\n",
       "4      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9217f",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f9b88",
   "metadata": {},
   "source": [
    "Check_Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c117be01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b53da",
   "metadata": {},
   "source": [
    "Check_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4769ba5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label    False\n",
       "tweet    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27994e9b",
   "metadata": {},
   "source": [
    "Check_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e26e867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6705ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36afbf0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d244ff2",
   "metadata": {},
   "source": [
    "As baseline processing we will do so :\n",
    "\n",
    "1-check for mails to remove\n",
    "\n",
    "2-check for websites to remove\n",
    " \n",
    "3-remove non chars\n",
    "\n",
    "4-Normalization\n",
    "\n",
    "5-remove stop words\n",
    "\n",
    "6-Lemmitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb1f6639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet'].apply(lambda x: re.findall('\\S+@\\S+',x)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50a97c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         @user when a father is dysfunctional and is s...\n",
       "1        @user @user thanks for #lyft credit i can't us...\n",
       "2                                      bihday your majesty\n",
       "3        #model   i love u take with u all the time in ...\n",
       "4                   factsguide: society now    #motivation\n",
       "                               ...                        \n",
       "31956    off fishing tomorrow @user carnt wait first ti...\n",
       "31957    ate @user isz that youuu?ðððððð...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31961                     thank you @user for you follow  \n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet']=Data['tweet'].apply(lambda x: re.sub('\\S+@\\S+',' ',x))\n",
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b07458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet'].apply(lambda x: re.findall('http\\S+',x)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a28f3c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          user when a father is dysfunctional and is s...\n",
       "1         user  user thanks for  lyft credit i can t us...\n",
       "2                                      bihday your majesty\n",
       "3         model   i love u take with u all the time in ...\n",
       "4                   factsguide  society now     motivation\n",
       "                               ...                        \n",
       "31956    off fishing tomorrow  user carnt wait first ti...\n",
       "31957    ate  user isz that youuu                      ...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31961                     thank you  user for you follow  \n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet']=Data['tweet'].apply(lambda x:re.sub('[^A-Za-z0-9]',' ',x))\n",
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "471fe3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['tweet']=Data['tweet'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b508360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          user when a father is dysfunctional and is s...\n",
       "1         user  user thanks for  lyft credit i can t us...\n",
       "2                                      bihday your majesty\n",
       "3         model   i love u take with u all the time in ...\n",
       "4                   factsguide  society now     motivation\n",
       "                               ...                        \n",
       "31956    off fishing tomorrow  user carnt wait first ti...\n",
       "31957    ate  user isz that youuu                      ...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31961                     thank you  user for you follow  \n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9660abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          user when a father is dysfunctional and is s...\n",
       "1         user  user thanks for  lyft credit i can t us...\n",
       "2                                      bihday your majesty\n",
       "3         model   i love u take with u all the time in ...\n",
       "4                   factsguide  society now     motivation\n",
       "                               ...                        \n",
       "31956    off fishing tomorrow  user carnt wait first ti...\n",
       "31957    ate  user isz that youuu                      ...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31961                     thank you  user for you follow  \n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet']=Data['tweet'].apply(lambda x: re.sub('\\d+','',x))\n",
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d48175db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Honda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7623acf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [user, father, dysfunctional, selfish, drags, ...\n",
       "1        [user, user, thanks, lyft, credit, use, cause,...\n",
       "2                                        [bihday, majesty]\n",
       "3                      [model, love, u, take, u, time, ur]\n",
       "4                        [factsguide, society, motivation]\n",
       "                               ...                        \n",
       "31956    [fishing, tomorrow, user, carnt, wait, first, ...\n",
       "31957                              [ate, user, isz, youuu]\n",
       "31958    [see, nina, turner, airwaves, trying, wrap, ma...\n",
       "31959    [listening, sad, songs, monday, morning, otw, ...\n",
       "31961                                [thank, user, follow]\n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet']=Data['tweet'].apply(lambda x : [word for word in x.split()  if word not in stop_words])\n",
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f7e49d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [user, father, dysfunctional, selfish, drag, k...\n",
       "1        [user, user, thank, lyft, credit, use, cause, ...\n",
       "2                                        [bihday, majesty]\n",
       "3                      [model, love, u, take, u, time, ur]\n",
       "4                        [factsguide, society, motivation]\n",
       "                               ...                        \n",
       "31956    [fish, tomorrow, user, carnt, wait, first, tim...\n",
       "31957                              [eat, user, isz, youuu]\n",
       "31958    [see, nina, turner, airwave, try, wrap, mantle...\n",
       "31959    [listen, sad, song, monday, morning, otw, work...\n",
       "31961                                [thank, user, follow]\n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags=[wordnet.VERB,wordnet.ADJ,wordnet.ADV,wordnet.NOUN]\n",
    "lemmitaizer=WordNetLemmatizer()\n",
    "for pos in pos_tags:\n",
    "    Data['tweet']=Data['tweet'].apply(lambda x: [lemmitaizer.lemmatize(word,pos=pos) for word in x])\n",
    "    \n",
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd8f0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['tweet']=Data['tweet'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "caa4bec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        user father dysfunctional selfish drag kid dys...\n",
       "1        user user thank lyft credit use cause offer wh...\n",
       "2                                           bihday majesty\n",
       "3                              model love u take u time ur\n",
       "4                            factsguide society motivation\n",
       "                               ...                        \n",
       "31956        fish tomorrow user carnt wait first time year\n",
       "31957                                   eat user isz youuu\n",
       "31958    see nina turner airwave try wrap mantle genuin...\n",
       "31959          listen sad song monday morning otw work sad\n",
       "31961                                    thank user follow\n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff2e76",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3751e4",
   "metadata": {},
   "source": [
    "1-count vectorizer\n",
    "\n",
    "2-tfidf\n",
    "\n",
    "3-tokenizer(binary,count,freq)\n",
    "\n",
    "4-pretrained(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0509ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target=Data['label']\n",
    "x_train, x_test ,y_train ,y_test = train_test_split(Data['tweet'],target,test_size=0.1,stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e417532",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b300b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=10000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer(max_features=10000,ngram_range=(1,1))\n",
    "count_vect.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "355e88bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9924"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_['york']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4df3d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Honda\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "287283a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count=count_vect.transform(x_train).todense()\n",
    "X_test_count=count_vect.transform(x_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67fc0e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Honda\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aap</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abasel</th>\n",
       "      <th>abba</th>\n",
       "      <th>abc</th>\n",
       "      <th>abe</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoological</th>\n",
       "      <th>zootopia</th>\n",
       "      <th>zoro</th>\n",
       "      <th>zosh</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zydeco</th>\n",
       "      <th>zzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user user monroe amp nick best friend amp rosalee amp addie kelly amp babymonrosale grimm family nadalind</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user real sweet successful</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tomorrow introduce world ebony long journey follow dream somethingnew</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user hope day special af bihday goodvibes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>em excite en hide</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>turn cheek aka religious freedom could mean responder choose respond right</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user gr loss condolence jo family yorkshire batley birstall jocoxmp sky brendancox brighton heave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user amp sick fuck look like pedophile friend user</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user user talk greenwood tulsa ok city tulsa amp state ok yet acknowledge massacre history</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ayu local friend girl night funabashi chiba</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26577 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    aa  aap  aaron  ab  \\\n",
       "user user monroe amp nick best friend amp rosal...   0    0      0   0   \n",
       "user real sweet successful                           0    0      0   0   \n",
       "tomorrow introduce world ebony long journey fol...   0    0      0   0   \n",
       "user hope day special af bihday goodvibes            0    0      0   0   \n",
       "em excite en hide                                    0    0      0   0   \n",
       "...                                                 ..  ...    ...  ..   \n",
       "turn cheek aka religious freedom could mean res...   0    0      0   0   \n",
       "user gr loss condolence jo family yorkshire bat...   0    0      0   0   \n",
       "user amp sick fuck look like pedophile friend user   0    0      0   0   \n",
       "user user talk greenwood tulsa ok city tulsa am...   0    0      0   0   \n",
       "ayu local friend girl night funabashi chiba          0    0      0   0   \n",
       "\n",
       "                                                    abandon  abasel  abba  \\\n",
       "user user monroe amp nick best friend amp rosal...        0       0     0   \n",
       "user real sweet successful                                0       0     0   \n",
       "tomorrow introduce world ebony long journey fol...        0       0     0   \n",
       "user hope day special af bihday goodvibes                 0       0     0   \n",
       "em excite en hide                                         0       0     0   \n",
       "...                                                     ...     ...   ...   \n",
       "turn cheek aka religious freedom could mean res...        0       0     0   \n",
       "user gr loss condolence jo family yorkshire bat...        0       0     0   \n",
       "user amp sick fuck look like pedophile friend user        0       0     0   \n",
       "user user talk greenwood tulsa ok city tulsa am...        0       0     0   \n",
       "ayu local friend girl night funabashi chiba               0       0     0   \n",
       "\n",
       "                                                    abc  abe  ability  ...  \\\n",
       "user user monroe amp nick best friend amp rosal...    0    0        0  ...   \n",
       "user real sweet successful                            0    0        0  ...   \n",
       "tomorrow introduce world ebony long journey fol...    0    0        0  ...   \n",
       "user hope day special af bihday goodvibes             0    0        0  ...   \n",
       "em excite en hide                                     0    0        0  ...   \n",
       "...                                                 ...  ...      ...  ...   \n",
       "turn cheek aka religious freedom could mean res...    0    0        0  ...   \n",
       "user gr loss condolence jo family yorkshire bat...    0    0        0  ...   \n",
       "user amp sick fuck look like pedophile friend user    0    0        0  ...   \n",
       "user user talk greenwood tulsa ok city tulsa am...    0    0        0  ...   \n",
       "ayu local friend girl night funabashi chiba           0    0        0  ...   \n",
       "\n",
       "                                                    zoo  zoological  zootopia  \\\n",
       "user user monroe amp nick best friend amp rosal...    0           0         0   \n",
       "user real sweet successful                            0           0         0   \n",
       "tomorrow introduce world ebony long journey fol...    0           0         0   \n",
       "user hope day special af bihday goodvibes             0           0         0   \n",
       "em excite en hide                                     0           0         0   \n",
       "...                                                 ...         ...       ...   \n",
       "turn cheek aka religious freedom could mean res...    0           0         0   \n",
       "user gr loss condolence jo family yorkshire bat...    0           0         0   \n",
       "user amp sick fuck look like pedophile friend user    0           0         0   \n",
       "user user talk greenwood tulsa ok city tulsa am...    0           0         0   \n",
       "ayu local friend girl night funabashi chiba           0           0         0   \n",
       "\n",
       "                                                    zoro  zosh  zucchini  \\\n",
       "user user monroe amp nick best friend amp rosal...     0     0         0   \n",
       "user real sweet successful                             0     0         0   \n",
       "tomorrow introduce world ebony long journey fol...     0     0         0   \n",
       "user hope day special af bihday goodvibes              0     0         0   \n",
       "em excite en hide                                      0     0         0   \n",
       "...                                                  ...   ...       ...   \n",
       "turn cheek aka religious freedom could mean res...     0     0         0   \n",
       "user gr loss condolence jo family yorkshire bat...     0     0         0   \n",
       "user amp sick fuck look like pedophile friend user     0     0         0   \n",
       "user user talk greenwood tulsa ok city tulsa am...     0     0         0   \n",
       "ayu local friend girl night funabashi chiba            0     0         0   \n",
       "\n",
       "                                                    zuma  zurich  zydeco  \\\n",
       "user user monroe amp nick best friend amp rosal...     0       0       0   \n",
       "user real sweet successful                             0       0       0   \n",
       "tomorrow introduce world ebony long journey fol...     0       0       0   \n",
       "user hope day special af bihday goodvibes              0       0       0   \n",
       "em excite en hide                                      0       0       0   \n",
       "...                                                  ...     ...     ...   \n",
       "turn cheek aka religious freedom could mean res...     0       0       0   \n",
       "user gr loss condolence jo family yorkshire bat...     0       0       0   \n",
       "user amp sick fuck look like pedophile friend user     0       0       0   \n",
       "user user talk greenwood tulsa ok city tulsa am...     0       0       0   \n",
       "ayu local friend girl night funabashi chiba            0       0       0   \n",
       "\n",
       "                                                    zzzzzzzz  \n",
       "user user monroe amp nick best friend amp rosal...         0  \n",
       "user real sweet successful                                 0  \n",
       "tomorrow introduce world ebony long journey fol...         0  \n",
       "user hope day special af bihday goodvibes                  0  \n",
       "em excite en hide                                          0  \n",
       "...                                                      ...  \n",
       "turn cheek aka religious freedom could mean res...         0  \n",
       "user gr loss condolence jo family yorkshire bat...         0  \n",
       "user amp sick fuck look like pedophile friend user         0  \n",
       "user user talk greenwood tulsa ok city tulsa am...         0  \n",
       "ayu local friend girl night funabashi chiba                0  \n",
       "\n",
       "[26577 rows x 10000 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_count,index=x_train,columns=count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "215f1702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26577, 10000)\n",
      "(2953, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_count.shape)\n",
    "print(X_test_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f29c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers,models\n",
    "model_1=models.Sequential()\n",
    "model_1.add(layers.Dense(128,activation='relu',input_shape=(10000,)))\n",
    "model_1.add(layers.Dense(64,activation='relu'))\n",
    "model_1.add(layers.Dense(32,activation='relu'))\n",
    "model_1.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b223638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Honda\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "model_1.compile(optimizer= RMSprop(lr=0.0001),\n",
    "              loss= keras.losses.binary_crossentropy,\n",
    "              metrics= [keras.metrics.binary_accuracy,keras.metrics.Precision(),keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80125dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "831/831 [==============================] - 37s 44ms/step - loss: 0.2937 - binary_accuracy: 0.9299 - precision: 0.0938 - recall: 0.0033 - val_loss: 0.2025 - val_binary_accuracy: 0.9319 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/10\n",
      "831/831 [==============================] - 35s 42ms/step - loss: 0.1861 - binary_accuracy: 0.9320 - precision: 0.8333 - recall: 0.0028 - val_loss: 0.1902 - val_binary_accuracy: 0.9329 - val_precision: 1.0000 - val_recall: 0.0149\n",
      "Epoch 3/10\n",
      "831/831 [==============================] - 37s 45ms/step - loss: 0.1734 - binary_accuracy: 0.9381 - precision: 0.8615 - recall: 0.1098 - val_loss: 0.1842 - val_binary_accuracy: 0.9414 - val_precision: 0.8333 - val_recall: 0.1741\n",
      "Epoch 4/10\n",
      "831/831 [==============================] - 36s 43ms/step - loss: 0.1646 - binary_accuracy: 0.9461 - precision: 0.8585 - recall: 0.2511 - val_loss: 0.1768 - val_binary_accuracy: 0.9462 - val_precision: 0.8500 - val_recall: 0.2537\n",
      "Epoch 5/10\n",
      "831/831 [==============================] - 36s 44ms/step - loss: 0.1538 - binary_accuracy: 0.9511 - precision: 0.8546 - recall: 0.3405 - val_loss: 0.1685 - val_binary_accuracy: 0.9472 - val_precision: 0.8000 - val_recall: 0.2985\n",
      "Epoch 6/10\n",
      "831/831 [==============================] - 35s 42ms/step - loss: 0.1427 - binary_accuracy: 0.9542 - precision: 0.8400 - recall: 0.4056 - val_loss: 0.1581 - val_binary_accuracy: 0.9502 - val_precision: 0.8068 - val_recall: 0.3532\n",
      "Epoch 7/10\n",
      "831/831 [==============================] - 38s 46ms/step - loss: 0.1317 - binary_accuracy: 0.9580 - precision: 0.8515 - recall: 0.4652 - val_loss: 0.1508 - val_binary_accuracy: 0.9523 - val_precision: 0.7941 - val_recall: 0.4030\n",
      "Epoch 8/10\n",
      "831/831 [==============================] - 36s 44ms/step - loss: 0.1232 - binary_accuracy: 0.9608 - precision: 0.8624 - recall: 0.5050 - val_loss: 0.1459 - val_binary_accuracy: 0.9543 - val_precision: 0.7946 - val_recall: 0.4428\n",
      "Epoch 9/10\n",
      "831/831 [==============================] - 36s 43ms/step - loss: 0.1158 - binary_accuracy: 0.9632 - precision: 0.8691 - recall: 0.5425 - val_loss: 0.1424 - val_binary_accuracy: 0.9546 - val_precision: 0.7769 - val_recall: 0.4677\n",
      "Epoch 10/10\n",
      "831/831 [==============================] - 37s 44ms/step - loss: 0.1096 - binary_accuracy: 0.9656 - precision: 0.8780 - recall: 0.5762 - val_loss: 0.1437 - val_binary_accuracy: 0.9556 - val_precision: 0.7966 - val_recall: 0.4677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21e5da51c70>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(X_train_count,y_train,epochs=10,validation_data=(X_test_count,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070bd9da",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c026d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=10000,oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d58bc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'user': 2,\n",
       " 'day': 3,\n",
       " 'love': 4,\n",
       " 'get': 5,\n",
       " 'happy': 6,\n",
       " 'amp': 7,\n",
       " 'go': 8,\n",
       " 'make': 9,\n",
       " 'life': 10,\n",
       " 'today': 11,\n",
       " 'u': 12,\n",
       " 'like': 13,\n",
       " 'good': 14,\n",
       " 'new': 15,\n",
       " 'father': 16,\n",
       " 'see': 17,\n",
       " 'time': 18,\n",
       " 'smile': 19,\n",
       " 'people': 20,\n",
       " 'bihday': 21,\n",
       " 'one': 22,\n",
       " 'friend': 23,\n",
       " 'feel': 24,\n",
       " 'come': 25,\n",
       " 'look': 26,\n",
       " 'work': 27,\n",
       " 'want': 28,\n",
       " 'wait': 29,\n",
       " 'girl': 30,\n",
       " 'thank': 31,\n",
       " 'weekend': 32,\n",
       " 'fun': 33,\n",
       " 'week': 34,\n",
       " 'think': 35,\n",
       " 'need': 36,\n",
       " 'family': 37,\n",
       " 'summer': 38,\n",
       " 'say': 39,\n",
       " 'great': 40,\n",
       " 'live': 41,\n",
       " 'know': 42,\n",
       " 'year': 43,\n",
       " 'friday': 44,\n",
       " 'thankful': 45,\n",
       " 'positive': 46,\n",
       " 'beautiful': 47,\n",
       " 'first': 48,\n",
       " 'morning': 49,\n",
       " 'back': 50,\n",
       " 'world': 51,\n",
       " 'take': 52,\n",
       " 'thing': 53,\n",
       " 'watch': 54,\n",
       " 'way': 55,\n",
       " 'dad': 56,\n",
       " 'tomorrow': 57,\n",
       " 'best': 58,\n",
       " 'home': 59,\n",
       " 'even': 60,\n",
       " 'really': 61,\n",
       " 'orlando': 62,\n",
       " 'sad': 63,\n",
       " 'sunday': 64,\n",
       " 'never': 65,\n",
       " 'music': 66,\n",
       " 'night': 67,\n",
       " 'fathersday': 68,\n",
       " 'blog': 69,\n",
       " 'cute': 70,\n",
       " 'right': 71,\n",
       " 'trump': 72,\n",
       " 'leave': 73,\n",
       " 'find': 74,\n",
       " 'much': 75,\n",
       " 'happiness': 76,\n",
       " 'play': 77,\n",
       " 'bless': 78,\n",
       " 'pay': 79,\n",
       " 'follow': 80,\n",
       " 'next': 81,\n",
       " 'show': 82,\n",
       " 'game': 83,\n",
       " 'last': 84,\n",
       " 'let': 85,\n",
       " 'amaze': 86,\n",
       " 'healthy': 87,\n",
       " 'always': 88,\n",
       " 'still': 89,\n",
       " 'give': 90,\n",
       " 'tonight': 91,\n",
       " 'gold': 92,\n",
       " 'guy': 93,\n",
       " 'selfie': 94,\n",
       " 'silver': 95,\n",
       " 'woman': 96,\n",
       " 'enjoy': 97,\n",
       " 'ready': 98,\n",
       " 'many': 99,\n",
       " 'little': 100,\n",
       " 'wish': 101,\n",
       " 'man': 102,\n",
       " 'everyone': 103,\n",
       " 'via': 104,\n",
       " 'excite': 105,\n",
       " 'quote': 106,\n",
       " 'bull': 107,\n",
       " 'keep': 108,\n",
       " 'miss': 109,\n",
       " 'baby': 110,\n",
       " 'lose': 111,\n",
       " 'well': 112,\n",
       " 'kid': 113,\n",
       " 'dog': 114,\n",
       " 'would': 115,\n",
       " 'god': 116,\n",
       " 'black': 117,\n",
       " 'try': 118,\n",
       " 'ever': 119,\n",
       " 'forex': 120,\n",
       " 'another': 121,\n",
       " 'bad': 122,\n",
       " 'finally': 123,\n",
       " 'book': 124,\n",
       " 'news': 125,\n",
       " 'big': 126,\n",
       " 'hope': 127,\n",
       " 'food': 128,\n",
       " 'sun': 129,\n",
       " 'white': 130,\n",
       " 'end': 131,\n",
       " 'sta': 132,\n",
       " 'call': 133,\n",
       " 'stop': 134,\n",
       " 'long': 135,\n",
       " 'shoot': 136,\n",
       " 'happen': 137,\n",
       " 'video': 138,\n",
       " 'every': 139,\n",
       " 'meet': 140,\n",
       " 'saturday': 141,\n",
       " 'hate': 142,\n",
       " 'hour': 143,\n",
       " 'place': 144,\n",
       " 'someone': 145,\n",
       " 'late': 146,\n",
       " 'th': 147,\n",
       " 'school': 148,\n",
       " 'holiday': 149,\n",
       " 'boy': 150,\n",
       " 'travel': 151,\n",
       " 'may': 152,\n",
       " 'change': 153,\n",
       " 'use': 154,\n",
       " 'believe': 155,\n",
       " 'help': 156,\n",
       " 'free': 157,\n",
       " 'n': 158,\n",
       " 'cool': 159,\n",
       " 'month': 160,\n",
       " 'break': 161,\n",
       " 'lol': 162,\n",
       " 'run': 163,\n",
       " 'fuck': 164,\n",
       " 'face': 165,\n",
       " 'yes': 166,\n",
       " 'funny': 167,\n",
       " 'bear': 168,\n",
       " 'child': 169,\n",
       " 'beach': 170,\n",
       " 'nothing': 171,\n",
       " 'read': 172,\n",
       " 'old': 173,\n",
       " 'nice': 174,\n",
       " 'hea': 175,\n",
       " 'euro': 176,\n",
       " 'w': 177,\n",
       " 'awesome': 178,\n",
       " 'team': 179,\n",
       " 'do': 180,\n",
       " 'shop': 181,\n",
       " 'word': 182,\n",
       " 'city': 183,\n",
       " 'race': 184,\n",
       " 'instagood': 185,\n",
       " 'please': 186,\n",
       " 'moment': 187,\n",
       " 'peace': 188,\n",
       " 'hear': 189,\n",
       " 'b': 190,\n",
       " 'gt': 191,\n",
       " 'true': 192,\n",
       " 'job': 193,\n",
       " 'monday': 194,\n",
       " 'lt': 195,\n",
       " 'kill': 196,\n",
       " 'r': 197,\n",
       " 'lot': 198,\n",
       " 'affirmation': 199,\n",
       " 'america': 200,\n",
       " 'tweet': 201,\n",
       " 'dream': 202,\n",
       " 'win': 203,\n",
       " 'check': 204,\n",
       " 'soon': 205,\n",
       " 'june': 206,\n",
       " 'proud': 207,\n",
       " 'wed': 208,\n",
       " 'dance': 209,\n",
       " 'two': 210,\n",
       " 'talk': 211,\n",
       " 'tell': 212,\n",
       " 'away': 213,\n",
       " 'around': 214,\n",
       " 'hair': 215,\n",
       " 'something': 216,\n",
       " 'forward': 217,\n",
       " 'stay': 218,\n",
       " 'person': 219,\n",
       " 'open': 220,\n",
       " 'attack': 221,\n",
       " 'lovely': 222,\n",
       " 'post': 223,\n",
       " 'hot': 224,\n",
       " 'motivation': 225,\n",
       " 'photo': 226,\n",
       " 'e': 227,\n",
       " 'fan': 228,\n",
       " 'real': 229,\n",
       " 'head': 230,\n",
       " 'buy': 231,\n",
       " 'oh': 232,\n",
       " 'mean': 233,\n",
       " 'climb': 234,\n",
       " 'season': 235,\n",
       " 'strong': 236,\n",
       " 'fashion': 237,\n",
       " 'hard': 238,\n",
       " 'without': 239,\n",
       " 'rip': 240,\n",
       " 'twitter': 241,\n",
       " 'cat': 242,\n",
       " 'gun': 243,\n",
       " 'song': 244,\n",
       " 'could': 245,\n",
       " 'relax': 246,\n",
       " 'cry': 247,\n",
       " 'sleep': 248,\n",
       " 'pa': 249,\n",
       " 'everything': 250,\n",
       " 'sex': 251,\n",
       " 'flower': 252,\n",
       " 'listen': 253,\n",
       " 'gay': 254,\n",
       " 'angry': 255,\n",
       " 'die': 256,\n",
       " 'young': 257,\n",
       " 'story': 258,\n",
       " 'share': 259,\n",
       " 'p': 260,\n",
       " 'wow': 261,\n",
       " 'racist': 262,\n",
       " 'house': 263,\n",
       " 'beauty': 264,\n",
       " 'drink': 265,\n",
       " 'put': 266,\n",
       " 'tear': 267,\n",
       " 'walk': 268,\n",
       " 'full': 269,\n",
       " 'gonna': 270,\n",
       " 'yet': 271,\n",
       " 'pretty': 272,\n",
       " 'movie': 273,\n",
       " 'london': 274,\n",
       " 'celebrate': 275,\n",
       " 'ur': 276,\n",
       " 'v': 277,\n",
       " 'mom': 278,\n",
       " 'st': 279,\n",
       " 'country': 280,\n",
       " 'men': 281,\n",
       " 'whatever': 282,\n",
       " 'obama': 283,\n",
       " 'im': 284,\n",
       " 'care': 285,\n",
       " 'order': 286,\n",
       " 'thursday': 287,\n",
       " 'couple': 288,\n",
       " 'truth': 289,\n",
       " 'close': 290,\n",
       " 'omg': 291,\n",
       " 'polar': 292,\n",
       " 'laugh': 293,\n",
       " 'move': 294,\n",
       " 'ticket': 295,\n",
       " 'sexy': 296,\n",
       " 'money': 297,\n",
       " 'already': 298,\n",
       " 'learn': 299,\n",
       " 'grateful': 300,\n",
       " 'mind': 301,\n",
       " 'joy': 302,\n",
       " 'shit': 303,\n",
       " 'eat': 304,\n",
       " 'health': 305,\n",
       " 'perfect': 306,\n",
       " 'become': 307,\n",
       " 'followme': 308,\n",
       " 'fitness': 309,\n",
       " 'high': 310,\n",
       " 'photooftheday': 311,\n",
       " 'turn': 312,\n",
       " 'bring': 313,\n",
       " 'poetry': 314,\n",
       " 'success': 315,\n",
       " 'forget': 316,\n",
       " 'plan': 317,\n",
       " 'pic': 318,\n",
       " 'wake': 319,\n",
       " 'medium': 320,\n",
       " 'rest': 321,\n",
       " 'finish': 322,\n",
       " 'dead': 323,\n",
       " 'must': 324,\n",
       " 'wonderful': 325,\n",
       " 'pm': 326,\n",
       " 'event': 327,\n",
       " 'enough': 328,\n",
       " 'mood': 329,\n",
       " 'arrive': 330,\n",
       " 'american': 331,\n",
       " 'business': 332,\n",
       " 'picture': 333,\n",
       " 'gift': 334,\n",
       " 'super': 335,\n",
       " 'x': 336,\n",
       " 'hey': 337,\n",
       " 'alone': 338,\n",
       " 'car': 339,\n",
       " 'since': 340,\n",
       " 'also': 341,\n",
       " 'train': 342,\n",
       " 'write': 343,\n",
       " 'sweet': 344,\n",
       " 'state': 345,\n",
       " 'top': 346,\n",
       " 'blue': 347,\n",
       " 'yay': 348,\n",
       " 'set': 349,\n",
       " 'son': 350,\n",
       " 'comment': 351,\n",
       " 'inspiration': 352,\n",
       " 'porn': 353,\n",
       " 'remember': 354,\n",
       " 'ppl': 355,\n",
       " 'eye': 356,\n",
       " 'pray': 357,\n",
       " 'together': 358,\n",
       " 'reason': 359,\n",
       " 'k': 360,\n",
       " 'matter': 361,\n",
       " 'begin': 362,\n",
       " 'ask': 363,\n",
       " 'till': 364,\n",
       " 'crazy': 365,\n",
       " 'rain': 366,\n",
       " 'kind': 367,\n",
       " 'le': 368,\n",
       " 'tbt': 369,\n",
       " 'almost': 370,\n",
       " 'choose': 371,\n",
       " 'trip': 372,\n",
       " 'vote': 373,\n",
       " 'body': 374,\n",
       " 'coffee': 375,\n",
       " 'depress': 376,\n",
       " 'wednesday': 377,\n",
       " 'lie': 378,\n",
       " 'send': 379,\n",
       " 'h': 380,\n",
       " 'fact': 381,\n",
       " 'wrong': 382,\n",
       " 'victim': 383,\n",
       " 'nude': 384,\n",
       " 'sunshine': 385,\n",
       " 'color': 386,\n",
       " 'update': 387,\n",
       " 'customer': 388,\n",
       " 'daddy': 389,\n",
       " 'yesterday': 390,\n",
       " 'minute': 391,\n",
       " 'light': 392,\n",
       " 'direct': 393,\n",
       " 'usa': 394,\n",
       " 'reach': 395,\n",
       " 'others': 396,\n",
       " 'parent': 397,\n",
       " 'tire': 398,\n",
       " 'follower': 399,\n",
       " 'cause': 400,\n",
       " 'point': 401,\n",
       " 'f': 402,\n",
       " 'far': 403,\n",
       " 'forever': 404,\n",
       " 'allahsoil': 405,\n",
       " 'daughter': 406,\n",
       " 'sign': 407,\n",
       " 'seem': 408,\n",
       " 'spend': 409,\n",
       " 'sorry': 410,\n",
       " 'gym': 411,\n",
       " 'sick': 412,\n",
       " 'prayfororlando': 413,\n",
       " 'early': 414,\n",
       " 'anything': 415,\n",
       " 'name': 416,\n",
       " 'nature': 417,\n",
       " 'hit': 418,\n",
       " 'else': 419,\n",
       " 'animal': 420,\n",
       " 'anymore': 421,\n",
       " 'green': 422,\n",
       " 'park': 423,\n",
       " 'gorilla': 424,\n",
       " 'hardcore': 425,\n",
       " 'bird': 426,\n",
       " 'act': 427,\n",
       " 'saw': 428,\n",
       " 'style': 429,\n",
       " 'thought': 430,\n",
       " 'speak': 431,\n",
       " 'actually': 432,\n",
       " 'dominate': 433,\n",
       " 'hand': 434,\n",
       " 'gbp': 435,\n",
       " 'uk': 436,\n",
       " 'favorite': 437,\n",
       " 'lady': 438,\n",
       " 'tuesday': 439,\n",
       " 'lgbt': 440,\n",
       " 'hello': 441,\n",
       " 'yeah': 442,\n",
       " 'lifestyle': 443,\n",
       " 'single': 444,\n",
       " 'nervous': 445,\n",
       " 'vacation': 446,\n",
       " 'bed': 447,\n",
       " 'cold': 448,\n",
       " 'special': 449,\n",
       " 'fly': 450,\n",
       " 'red': 451,\n",
       " 'suppo': 452,\n",
       " 'social': 453,\n",
       " 'date': 454,\n",
       " 'easy': 455,\n",
       " 'muslim': 456,\n",
       " 'tgif': 457,\n",
       " 'surprise': 458,\n",
       " 'side': 459,\n",
       " 'sister': 460,\n",
       " 'prayer': 461,\n",
       " 'design': 462,\n",
       " 'sometimes': 463,\n",
       " 'grow': 464,\n",
       " 'understand': 465,\n",
       " 'release': 466,\n",
       " 'star': 467,\n",
       " 'instagram': 468,\n",
       " 'hu': 469,\n",
       " 'chill': 470,\n",
       " 'join': 471,\n",
       " 'sure': 472,\n",
       " 'hold': 473,\n",
       " 'goal': 474,\n",
       " 'memory': 475,\n",
       " 'picoftheday': 476,\n",
       " 'model': 477,\n",
       " 'sit': 478,\n",
       " 'view': 479,\n",
       " 'death': 480,\n",
       " 'final': 481,\n",
       " 'bite': 482,\n",
       " 'goodmorning': 483,\n",
       " 'guess': 484,\n",
       " 'anyone': 485,\n",
       " 'lucky': 486,\n",
       " 'complete': 487,\n",
       " 'lunch': 488,\n",
       " 'garden': 489,\n",
       " 'daily': 490,\n",
       " 'depression': 491,\n",
       " 'app': 492,\n",
       " 'film': 493,\n",
       " 'dear': 494,\n",
       " 'student': 495,\n",
       " 'brother': 496,\n",
       " 'episode': 497,\n",
       " 'environment': 498,\n",
       " 'everyday': 499,\n",
       " 'ago': 500,\n",
       " 'usd': 501,\n",
       " 'present': 502,\n",
       " 'sunny': 503,\n",
       " 'football': 504,\n",
       " 'pas': 505,\n",
       " 'conference': 506,\n",
       " 'human': 507,\n",
       " 'sing': 508,\n",
       " 'power': 509,\n",
       " 'history': 510,\n",
       " 'brexit': 511,\n",
       " 'ok': 512,\n",
       " 'tv': 513,\n",
       " 'florida': 514,\n",
       " 'visit': 515,\n",
       " 'treat': 516,\n",
       " 'yr': 517,\n",
       " 'fight': 518,\n",
       " 'realize': 519,\n",
       " 'lead': 520,\n",
       " 'self': 521,\n",
       " 'racism': 522,\n",
       " 'joke': 523,\n",
       " 'stas': 524,\n",
       " 'future': 525,\n",
       " 'disney': 526,\n",
       " 'control': 527,\n",
       " 'c': 528,\n",
       " 'group': 529,\n",
       " 'might': 530,\n",
       " 'able': 531,\n",
       " 'youtube': 532,\n",
       " 'snapchat': 533,\n",
       " 'simulator': 534,\n",
       " 'adapt': 535,\n",
       " 'experience': 536,\n",
       " 'club': 537,\n",
       " 'fail': 538,\n",
       " 'fall': 539,\n",
       " 'police': 540,\n",
       " 'buffalo': 541,\n",
       " 'na': 542,\n",
       " 'bitch': 543,\n",
       " 'retweet': 544,\n",
       " 'friendship': 545,\n",
       " 'launch': 546,\n",
       " 'problem': 547,\n",
       " 'line': 548,\n",
       " 'soul': 549,\n",
       " 'la': 550,\n",
       " 'stand': 551,\n",
       " 'dont': 552,\n",
       " 'service': 553,\n",
       " 'mad': 554,\n",
       " 'empty': 555,\n",
       " 'welcome': 556,\n",
       " 'agree': 557,\n",
       " 'president': 558,\n",
       " 'anniversary': 559,\n",
       " 'whole': 560,\n",
       " 'photography': 561,\n",
       " 'nd': 562,\n",
       " 'disappoint': 563,\n",
       " 'inspire': 564,\n",
       " 'rock': 565,\n",
       " 'thankyou': 566,\n",
       " 'damn': 567,\n",
       " 'breakfast': 568,\n",
       " 'cheer': 569,\n",
       " 'idea': 570,\n",
       " 'afternoon': 571,\n",
       " 'though': 572,\n",
       " 'queen': 573,\n",
       " 'class': 574,\n",
       " 'pathetic': 575,\n",
       " 'conce': 576,\n",
       " 'decide': 577,\n",
       " 'as': 578,\n",
       " 'dinner': 579,\n",
       " 'simple': 580,\n",
       " 'tragedy': 581,\n",
       " 'respect': 582,\n",
       " 'create': 583,\n",
       " 'boyfriend': 584,\n",
       " 'cant': 585,\n",
       " 'fast': 586,\n",
       " 'list': 587,\n",
       " 'camp': 588,\n",
       " 'water': 589,\n",
       " 'test': 590,\n",
       " 'step': 591,\n",
       " 'phone': 592,\n",
       " 'add': 593,\n",
       " 'staing': 594,\n",
       " 'heal': 595,\n",
       " 'ramadan': 596,\n",
       " 'save': 597,\n",
       " 'sky': 598,\n",
       " 'cannot': 599,\n",
       " 'drive': 600,\n",
       " 'safe': 601,\n",
       " 'july': 602,\n",
       " 'nyc': 603,\n",
       " 'cantwait': 604,\n",
       " 'likelike': 605,\n",
       " 'sound': 606,\n",
       " 'issue': 607,\n",
       " 'fresh': 608,\n",
       " 'impoant': 609,\n",
       " 'cake': 610,\n",
       " 'ht': 611,\n",
       " 'count': 612,\n",
       " 'half': 613,\n",
       " 'low': 614,\n",
       " 'fire': 615,\n",
       " 'voice': 616,\n",
       " 'stuff': 617,\n",
       " 'orlandoshooting': 618,\n",
       " 'vine': 619,\n",
       " 'sunset': 620,\n",
       " 'target': 621,\n",
       " 'officially': 622,\n",
       " 'dj': 623,\n",
       " 'mother': 624,\n",
       " 'education': 625,\n",
       " 'card': 626,\n",
       " 'l': 627,\n",
       " 'shock': 628,\n",
       " 'sale': 629,\n",
       " 'smh': 630,\n",
       " 'beer': 631,\n",
       " 'different': 632,\n",
       " 'ride': 633,\n",
       " 'pain': 634,\n",
       " 'culture': 635,\n",
       " 'adventure': 636,\n",
       " 'yoga': 637,\n",
       " 'choice': 638,\n",
       " 'small': 639,\n",
       " 'festival': 640,\n",
       " 'wanna': 641,\n",
       " 'cover': 642,\n",
       " 'pick': 643,\n",
       " 'trend': 644,\n",
       " 'hr': 645,\n",
       " 'client': 646,\n",
       " 'dress': 647,\n",
       " 'mindset': 648,\n",
       " 'expect': 649,\n",
       " 'mine': 650,\n",
       " 'relationship': 651,\n",
       " 'sell': 652,\n",
       " 'least': 653,\n",
       " 'tag': 654,\n",
       " 'past': 655,\n",
       " 'fear': 656,\n",
       " 'wife': 657,\n",
       " 'maybe': 658,\n",
       " 'bag': 659,\n",
       " 'simulation': 660,\n",
       " 'war': 661,\n",
       " 'draw': 662,\n",
       " 'pack': 663,\n",
       " 'course': 664,\n",
       " 'workout': 665,\n",
       " 'mass': 666,\n",
       " 'flight': 667,\n",
       " 'india': 668,\n",
       " 'community': 669,\n",
       " 'makeup': 670,\n",
       " 'england': 671,\n",
       " 'age': 672,\n",
       " 'reality': 673,\n",
       " 'beat': 674,\n",
       " 'cut': 675,\n",
       " 'oitnb': 676,\n",
       " 'htt': 677,\n",
       " 'web': 678,\n",
       " 'pop': 679,\n",
       " 'hillary': 680,\n",
       " 'question': 681,\n",
       " 'g': 682,\n",
       " 'kick': 683,\n",
       " 'teen': 684,\n",
       " 'wtf': 685,\n",
       " 'actor': 686,\n",
       " 'build': 687,\n",
       " 'nigga': 688,\n",
       " 'murder': 689,\n",
       " 'wonder': 690,\n",
       " 'dark': 691,\n",
       " 'truly': 692,\n",
       " 'fridayfeeling': 693,\n",
       " 'law': 694,\n",
       " 'ff': 695,\n",
       " 'ahead': 696,\n",
       " 'xxx': 697,\n",
       " 'sea': 698,\n",
       " 'instalike': 699,\n",
       " 'politics': 700,\n",
       " 'deserve': 701,\n",
       " 'return': 702,\n",
       " 'bday': 703,\n",
       " 'vibe': 704,\n",
       " 'lonely': 705,\n",
       " 'wet': 706,\n",
       " 'japan': 707,\n",
       " 'violence': 708,\n",
       " 'mr': 709,\n",
       " 'despite': 710,\n",
       " 'wear': 711,\n",
       " 'room': 712,\n",
       " 'street': 713,\n",
       " 'account': 714,\n",
       " 'receive': 715,\n",
       " 'pink': 716,\n",
       " 'unite': 717,\n",
       " 'liberal': 718,\n",
       " 'content': 719,\n",
       " 'fit': 720,\n",
       " 'blur': 721,\n",
       " 'three': 722,\n",
       " 'blonde': 723,\n",
       " 'hero': 724,\n",
       " 'emotion': 725,\n",
       " 'scar': 726,\n",
       " 'product': 727,\n",
       " 'leadership': 728,\n",
       " 'message': 729,\n",
       " 'congrats': 730,\n",
       " 'shoe': 731,\n",
       " 'weather': 732,\n",
       " 'result': 733,\n",
       " 'market': 734,\n",
       " 'poor': 735,\n",
       " 'deal': 736,\n",
       " 'instead': 737,\n",
       " 'record': 738,\n",
       " 'clean': 739,\n",
       " 'local': 740,\n",
       " 'office': 741,\n",
       " 'apple': 742,\n",
       " 'nbafinals': 743,\n",
       " 'catch': 744,\n",
       " 'husband': 745,\n",
       " 'behind': 746,\n",
       " 'disgust': 747,\n",
       " 'gop': 748,\n",
       " 'bore': 749,\n",
       " 'second': 750,\n",
       " 'sho': 751,\n",
       " 'stupid': 752,\n",
       " 'wine': 753,\n",
       " 'rather': 754,\n",
       " 'worry': 755,\n",
       " 'continue': 756,\n",
       " 'freedom': 757,\n",
       " 'pizza': 758,\n",
       " 'fantastic': 759,\n",
       " 'puppy': 760,\n",
       " 'survive': 761,\n",
       " 'trust': 762,\n",
       " 'drop': 763,\n",
       " 'land': 764,\n",
       " 'announce': 765,\n",
       " 'tragic': 766,\n",
       " 'interest': 767,\n",
       " 'france': 768,\n",
       " 'gorgeous': 769,\n",
       " 'busy': 770,\n",
       " 'hat': 771,\n",
       " 'loss': 772,\n",
       " 'heabroken': 773,\n",
       " 'tip': 774,\n",
       " 'woh': 775,\n",
       " 'gotta': 776,\n",
       " 'online': 777,\n",
       " 'award': 778,\n",
       " 'deep': 779,\n",
       " 'along': 780,\n",
       " 'political': 781,\n",
       " 'staed': 782,\n",
       " 'countdown': 783,\n",
       " 'hatred': 784,\n",
       " 'celebration': 785,\n",
       " 'pussy': 786,\n",
       " 'national': 787,\n",
       " 'folk': 788,\n",
       " 'series': 789,\n",
       " 'ripchristina': 790,\n",
       " 'luck': 791,\n",
       " 'york': 792,\n",
       " 'swim': 793,\n",
       " 'glad': 794,\n",
       " 'instamood': 795,\n",
       " 'yo': 796,\n",
       " 'official': 797,\n",
       " 'seriously': 798,\n",
       " 'accept': 799,\n",
       " 'co': 800,\n",
       " 'block': 801,\n",
       " 'hungry': 802,\n",
       " 'ice': 803,\n",
       " 'allow': 804,\n",
       " 'suck': 805,\n",
       " 'energy': 806,\n",
       " 'glass': 807,\n",
       " 'pride': 808,\n",
       " 'inshot': 809,\n",
       " 'page': 810,\n",
       " 'road': 811,\n",
       " 'offer': 812,\n",
       " 'brand': 813,\n",
       " 'oil': 814,\n",
       " 'hi': 815,\n",
       " 'congratulation': 816,\n",
       " 'spos': 817,\n",
       " 'blame': 818,\n",
       " 'asian': 819,\n",
       " 'tech': 820,\n",
       " 'due': 821,\n",
       " 'germany': 822,\n",
       " 'fill': 823,\n",
       " 'blogger': 824,\n",
       " 'album': 825,\n",
       " 'campaign': 826,\n",
       " 'islam': 827,\n",
       " 'literally': 828,\n",
       " 'haha': 829,\n",
       " 'college': 830,\n",
       " 'facebook': 831,\n",
       " 'member': 832,\n",
       " 'project': 833,\n",
       " 'bike': 834,\n",
       " 'mountain': 835,\n",
       " 'appreciate': 836,\n",
       " 'hill': 837,\n",
       " 'review': 838,\n",
       " 'paint': 839,\n",
       " 'king': 840,\n",
       " 'stick': 841,\n",
       " 'cook': 842,\n",
       " 'waste': 843,\n",
       " 'action': 844,\n",
       " 'pool': 845,\n",
       " 'remain': 846,\n",
       " 'secret': 847,\n",
       " 'nasty': 848,\n",
       " 'answer': 849,\n",
       " 'paris': 850,\n",
       " 'staff': 851,\n",
       " 'confuse': 852,\n",
       " 'foot': 853,\n",
       " 'pet': 854,\n",
       " 'chase': 855,\n",
       " 'number': 856,\n",
       " 'donald': 857,\n",
       " 'inside': 858,\n",
       " 'female': 859,\n",
       " 'tune': 860,\n",
       " 'key': 861,\n",
       " 'shame': 862,\n",
       " 'church': 863,\n",
       " 'company': 864,\n",
       " 'store': 865,\n",
       " 'town': 866,\n",
       " 'space': 867,\n",
       " 'weak': 868,\n",
       " 'vegan': 869,\n",
       " 'terrorist': 870,\n",
       " 'link': 871,\n",
       " 'hop': 872,\n",
       " 'nation': 873,\n",
       " 'clothe': 874,\n",
       " 'fake': 875,\n",
       " 'miami': 876,\n",
       " 'hell': 877,\n",
       " 'christmas': 878,\n",
       " 'bc': 879,\n",
       " 'huge': 880,\n",
       " 'tour': 881,\n",
       " 'leader': 882,\n",
       " 'instadaily': 883,\n",
       " 'min': 884,\n",
       " 'nofilter': 885,\n",
       " 'rd': 886,\n",
       " 'source': 887,\n",
       " 'yummy': 888,\n",
       " 'everybody': 889,\n",
       " 'naked': 890,\n",
       " 'marry': 891,\n",
       " 'okay': 892,\n",
       " 'teach': 893,\n",
       " 'box': 894,\n",
       " 'case': 895,\n",
       " 'upset': 896,\n",
       " 'spain': 897,\n",
       " 'session': 898,\n",
       " 'udtapunjab': 899,\n",
       " 'stage': 900,\n",
       " 'naughty': 901,\n",
       " 'dude': 902,\n",
       " 'calm': 903,\n",
       " 'idiot': 904,\n",
       " 'prove': 905,\n",
       " 'stream': 906,\n",
       " 'notice': 907,\n",
       " 'anti': 908,\n",
       " 'da': 909,\n",
       " 'internet': 910,\n",
       " 'vega': 911,\n",
       " 'previous': 912,\n",
       " 'balance': 913,\n",
       " 'gif': 914,\n",
       " 'chance': 915,\n",
       " 'nobody': 916,\n",
       " 'poem': 917,\n",
       " 'favourite': 918,\n",
       " 'study': 919,\n",
       " 'slut': 920,\n",
       " 'pulse': 921,\n",
       " 'loveit': 922,\n",
       " 'kiss': 923,\n",
       " 'ibiza': 924,\n",
       " 'faith': 925,\n",
       " 'girlfriend': 926,\n",
       " 'goodnight': 927,\n",
       " 'development': 928,\n",
       " 'coach': 929,\n",
       " 'board': 930,\n",
       " 'sense': 931,\n",
       " 'piece': 932,\n",
       " 'shy': 933,\n",
       " 'alive': 934,\n",
       " 'totally': 935,\n",
       " 'august': 936,\n",
       " 'legend': 937,\n",
       " 'youth': 938,\n",
       " 'nba': 939,\n",
       " 'bar': 940,\n",
       " 'gratitude': 941,\n",
       " 'generation': 942,\n",
       " 'roll': 943,\n",
       " 'absolutely': 944,\n",
       " 'motivate': 945,\n",
       " 'email': 946,\n",
       " 'hug': 947,\n",
       " 'melancholy': 948,\n",
       " 'tea': 949,\n",
       " 'horny': 950,\n",
       " 'available': 951,\n",
       " 'dory': 952,\n",
       " 'task': 953,\n",
       " 'hang': 954,\n",
       " 'million': 955,\n",
       " 'type': 956,\n",
       " 'xx': 957,\n",
       " 'ball': 958,\n",
       " 'pre': 959,\n",
       " 'either': 960,\n",
       " 'public': 961,\n",
       " 'probably': 962,\n",
       " 'positivity': 963,\n",
       " 'focus': 964,\n",
       " 'goodvibes': 965,\n",
       " 'player': 966,\n",
       " 'aist': 967,\n",
       " 'ex': 968,\n",
       " 'successful': 969,\n",
       " 'claim': 970,\n",
       " 'outside': 971,\n",
       " 'price': 972,\n",
       " 'term': 973,\n",
       " 'exactly': 974,\n",
       " 'lord': 975,\n",
       " 'emotional': 976,\n",
       " 'eur': 977,\n",
       " 'cavs': 978,\n",
       " 'towards': 979,\n",
       " 'lovelife': 980,\n",
       " 'match': 981,\n",
       " 'bride': 982,\n",
       " 'website': 983,\n",
       " 'republican': 984,\n",
       " 'exam': 985,\n",
       " 'russia': 986,\n",
       " 'loser': 987,\n",
       " 'especially': 988,\n",
       " 'anger': 989,\n",
       " 'de': 990,\n",
       " 'role': 991,\n",
       " 'press': 992,\n",
       " 'netflix': 993,\n",
       " 'christian': 994,\n",
       " 'kinky': 995,\n",
       " 'profile': 996,\n",
       " 'door': 997,\n",
       " 'ness': 998,\n",
       " 'note': 999,\n",
       " 'eah': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bda4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokenized=tokenizer.texts_to_matrix(x_train,mode='binary')\n",
    "x_test_tokenized=tokenizer.texts_to_matrix(x_test,mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29f009e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9990</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>come westhaven potchfest see brother user play pm today promise great</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rasebud interior instacool smile style like follow tbt fun giditraffic user</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bday entourage love instagood itsmybihday u cute follow photooftheday</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nigeria try help son weneedhelp youareaparenttoo</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cheeky write session user</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n healthy take much surprise others surprise oneself great feat kristen haley twtl</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saturday night live yay high five cecily strong bobby moynihan mc donalds</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user congratulation big guy put moan long</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user b coz write sho review fav podcast user user publish user</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aw elijah block hu cry</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26577 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0     1     2     3     \\\n",
       "come westhaven potchfest see brother user play ...   0.0   1.0   1.0   0.0   \n",
       "rasebud interior instacool smile style like fol...   0.0   1.0   1.0   0.0   \n",
       "bday entourage love instagood itsmybihday u cut...   0.0   1.0   0.0   0.0   \n",
       "nigeria try help son weneedhelp youareaparenttoo     0.0   1.0   0.0   0.0   \n",
       "cheeky write session user                            0.0   0.0   1.0   0.0   \n",
       "...                                                  ...   ...   ...   ...   \n",
       "n healthy take much surprise others surprise on...   0.0   1.0   0.0   0.0   \n",
       "saturday night live yay high five cecily strong...   0.0   1.0   0.0   0.0   \n",
       "user congratulation big guy put moan long            0.0   0.0   1.0   0.0   \n",
       "user b coz write sho review fav podcast user us...   0.0   0.0   1.0   0.0   \n",
       "aw elijah block hu cry                               0.0   1.0   0.0   0.0   \n",
       "\n",
       "                                                    4     5     6     7     \\\n",
       "come westhaven potchfest see brother user play ...   0.0   0.0   0.0   0.0   \n",
       "rasebud interior instacool smile style like fol...   0.0   0.0   0.0   0.0   \n",
       "bday entourage love instagood itsmybihday u cut...   1.0   0.0   0.0   0.0   \n",
       "nigeria try help son weneedhelp youareaparenttoo     0.0   0.0   0.0   0.0   \n",
       "cheeky write session user                            0.0   0.0   0.0   0.0   \n",
       "...                                                  ...   ...   ...   ...   \n",
       "n healthy take much surprise others surprise on...   0.0   0.0   0.0   0.0   \n",
       "saturday night live yay high five cecily strong...   0.0   0.0   0.0   0.0   \n",
       "user congratulation big guy put moan long            0.0   0.0   0.0   0.0   \n",
       "user b coz write sho review fav podcast user us...   0.0   0.0   0.0   0.0   \n",
       "aw elijah block hu cry                               0.0   0.0   0.0   0.0   \n",
       "\n",
       "                                                    8     9     ...  9990  \\\n",
       "come westhaven potchfest see brother user play ...   0.0   0.0  ...   0.0   \n",
       "rasebud interior instacool smile style like fol...   0.0   0.0  ...   0.0   \n",
       "bday entourage love instagood itsmybihday u cut...   0.0   0.0  ...   0.0   \n",
       "nigeria try help son weneedhelp youareaparenttoo     0.0   0.0  ...   0.0   \n",
       "cheeky write session user                            0.0   0.0  ...   0.0   \n",
       "...                                                  ...   ...  ...   ...   \n",
       "n healthy take much surprise others surprise on...   0.0   0.0  ...   0.0   \n",
       "saturday night live yay high five cecily strong...   0.0   0.0  ...   0.0   \n",
       "user congratulation big guy put moan long            0.0   0.0  ...   0.0   \n",
       "user b coz write sho review fav podcast user us...   0.0   0.0  ...   0.0   \n",
       "aw elijah block hu cry                               0.0   0.0  ...   0.0   \n",
       "\n",
       "                                                    9991  9992  9993  9994  \\\n",
       "come westhaven potchfest see brother user play ...   0.0   0.0   0.0   0.0   \n",
       "rasebud interior instacool smile style like fol...   0.0   0.0   0.0   0.0   \n",
       "bday entourage love instagood itsmybihday u cut...   0.0   0.0   0.0   0.0   \n",
       "nigeria try help son weneedhelp youareaparenttoo     0.0   0.0   0.0   0.0   \n",
       "cheeky write session user                            0.0   0.0   0.0   0.0   \n",
       "...                                                  ...   ...   ...   ...   \n",
       "n healthy take much surprise others surprise on...   0.0   0.0   0.0   0.0   \n",
       "saturday night live yay high five cecily strong...   0.0   0.0   0.0   0.0   \n",
       "user congratulation big guy put moan long            0.0   0.0   0.0   0.0   \n",
       "user b coz write sho review fav podcast user us...   0.0   0.0   0.0   0.0   \n",
       "aw elijah block hu cry                               0.0   0.0   0.0   0.0   \n",
       "\n",
       "                                                    9995  9996  9997  9998  \\\n",
       "come westhaven potchfest see brother user play ...   0.0   0.0   0.0   0.0   \n",
       "rasebud interior instacool smile style like fol...   0.0   0.0   0.0   0.0   \n",
       "bday entourage love instagood itsmybihday u cut...   0.0   0.0   0.0   0.0   \n",
       "nigeria try help son weneedhelp youareaparenttoo     0.0   0.0   0.0   0.0   \n",
       "cheeky write session user                            0.0   0.0   0.0   0.0   \n",
       "...                                                  ...   ...   ...   ...   \n",
       "n healthy take much surprise others surprise on...   0.0   0.0   0.0   0.0   \n",
       "saturday night live yay high five cecily strong...   0.0   0.0   0.0   0.0   \n",
       "user congratulation big guy put moan long            0.0   0.0   0.0   0.0   \n",
       "user b coz write sho review fav podcast user us...   0.0   0.0   0.0   0.0   \n",
       "aw elijah block hu cry                               0.0   0.0   0.0   0.0   \n",
       "\n",
       "                                                    9999  \n",
       "come westhaven potchfest see brother user play ...   0.0  \n",
       "rasebud interior instacool smile style like fol...   0.0  \n",
       "bday entourage love instagood itsmybihday u cut...   0.0  \n",
       "nigeria try help son weneedhelp youareaparenttoo     0.0  \n",
       "cheeky write session user                            0.0  \n",
       "...                                                  ...  \n",
       "n healthy take much surprise others surprise on...   0.0  \n",
       "saturday night live yay high five cecily strong...   0.0  \n",
       "user congratulation big guy put moan long            0.0  \n",
       "user b coz write sho review fav podcast user us...   0.0  \n",
       "aw elijah block hu cry                               0.0  \n",
       "\n",
       "[26577 rows x 10000 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train_tokenized,index=x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23323a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers,models\n",
    "model_2=models.Sequential()\n",
    "model_2.add(layers.Dense(128,activation='relu',input_shape=(10000,)))\n",
    "model_2.add(layers.Dense(64,activation='relu'))\n",
    "model_2.add(layers.Dense(32,activation='relu'))\n",
    "model_2.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fb99818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Honda\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "model_2.compile(optimizer= RMSprop(lr=0.0005),\n",
    "              loss= keras.losses.binary_crossentropy,\n",
    "              metrics= [keras.metrics.binary_accuracy,keras.metrics.Precision(),keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20435553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "831/831 [==============================] - 14s 16ms/step - loss: 0.0014 - binary_accuracy: 0.9997 - precision_4: 1.0000 - recall_4: 0.9961 - val_loss: 0.9370 - val_binary_accuracy: 0.9607 - val_precision_4: 0.7852 - val_recall_4: 0.5821\n",
      "Epoch 2/10\n",
      "831/831 [==============================] - 13s 15ms/step - loss: 0.0012 - binary_accuracy: 0.9997 - precision_4: 1.0000 - recall_4: 0.9961 - val_loss: 1.1279 - val_binary_accuracy: 0.9624 - val_precision_4: 0.8261 - val_recall_4: 0.5672\n",
      "Epoch 3/10\n",
      "831/831 [==============================] - 12s 15ms/step - loss: 0.0028 - binary_accuracy: 0.9997 - precision_4: 1.0000 - recall_4: 0.9961 - val_loss: 1.0297 - val_binary_accuracy: 0.9607 - val_precision_4: 0.7891 - val_recall_4: 0.5771\n",
      "Epoch 4/10\n",
      "831/831 [==============================] - 13s 15ms/step - loss: 0.0027 - binary_accuracy: 0.9997 - precision_4: 1.0000 - recall_4: 0.9961 - val_loss: 1.0227 - val_binary_accuracy: 0.9611 - val_precision_4: 0.7945 - val_recall_4: 0.5771\n",
      "Epoch 5/10\n",
      "831/831 [==============================] - 13s 16ms/step - loss: 0.0024 - binary_accuracy: 0.9997 - precision_4: 1.0000 - recall_4: 0.9961 - val_loss: 0.9939 - val_binary_accuracy: 0.9600 - val_precision_4: 0.7823 - val_recall_4: 0.5721\n",
      "Epoch 6/10\n",
      "831/831 [==============================] - 13s 16ms/step - loss: 0.0027 - binary_accuracy: 0.9997 - precision_4: 1.0000 - recall_4: 0.9961 - val_loss: 0.9630 - val_binary_accuracy: 0.9604 - val_precision_4: 0.7838 - val_recall_4: 0.5771\n",
      "Epoch 7/10\n",
      "831/831 [==============================] - 13s 15ms/step - loss: 0.0026 - binary_accuracy: 0.9997 - precision_4: 1.0000 - recall_4: 0.9961 - val_loss: 0.9184 - val_binary_accuracy: 0.9617 - val_precision_4: 0.8099 - val_recall_4: 0.5721\n",
      "Epoch 8/10\n",
      "831/831 [==============================] - 13s 16ms/step - loss: 0.0026 - binary_accuracy: 0.9997 - precision_4: 1.0000 - recall_4: 0.9961 - val_loss: 0.8696 - val_binary_accuracy: 0.9583 - val_precision_4: 0.7468 - val_recall_4: 0.5871\n",
      "Epoch 9/10\n",
      "831/831 [==============================] - 13s 15ms/step - loss: 0.0027 - binary_accuracy: 0.9997 - precision_4: 1.0000 - recall_4: 0.9961 - val_loss: 0.8401 - val_binary_accuracy: 0.9580 - val_precision_4: 0.7421 - val_recall_4: 0.5871\n",
      "Epoch 10/10\n",
      "831/831 [==============================] - 13s 16ms/step - loss: 0.0024 - binary_accuracy: 0.9997 - precision_4: 1.0000 - recall_4: 0.9961 - val_loss: 0.8320 - val_binary_accuracy: 0.9607 - val_precision_4: 0.7931 - val_recall_4: 0.5721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c9d5c8f40>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(x_train_tokenized,y_train,epochs=10,validation_data=(x_test_tokenized,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b02a7",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c4a5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f132568",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_v=np.zeros((len(x_train),300))\n",
    "x_test_v=np.zeros((len(x_test),300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c4a25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(nlp.pipe(x_train)):\n",
    "    x_train_v[i, :] = doc.vector\n",
    "\n",
    "for i, doc in enumerate(nlp.pipe(x_test)):\n",
    "    x_test_v[i, :] = doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58b9fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers,models\n",
    "model_3=models.Sequential()\n",
    "model_3.add(layers.Dense(128,activation='relu',input_shape=(300,)))\n",
    "model_3.add(layers.Dense(64,activation='relu'))\n",
    "model_3.add(layers.Dense(32,activation='relu'))\n",
    "model_3.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "705d5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "model_3.compile(optimizer= RMSprop(lr=0.0005),\n",
    "              loss= keras.losses.binary_crossentropy,\n",
    "              metrics= [keras.metrics.binary_accuracy,keras.metrics.Precision(),keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "308aa97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.0989 - binary_accuracy: 0.9692 - precision_6: 0.8828 - recall_6: 0.6319 - val_loss: 0.1981 - val_binary_accuracy: 0.9472 - val_precision_6: 0.6891 - val_recall_6: 0.4080\n",
      "Epoch 2/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.0944 - binary_accuracy: 0.9700 - precision_6: 0.8773 - recall_6: 0.6512 - val_loss: 0.2077 - val_binary_accuracy: 0.9434 - val_precision_6: 0.6000 - val_recall_6: 0.5075\n",
      "Epoch 3/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.0884 - binary_accuracy: 0.9725 - precision_6: 0.8978 - recall_6: 0.6738 - val_loss: 0.1976 - val_binary_accuracy: 0.9499 - val_precision_6: 0.7387 - val_recall_6: 0.4080\n",
      "Epoch 4/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.0847 - binary_accuracy: 0.9722 - precision_6: 0.8807 - recall_6: 0.6843 - val_loss: 0.2098 - val_binary_accuracy: 0.9468 - val_precision_6: 0.6549 - val_recall_6: 0.4627\n",
      "Epoch 5/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.0796 - binary_accuracy: 0.9753 - precision_6: 0.9070 - recall_6: 0.7103 - val_loss: 0.2396 - val_binary_accuracy: 0.9472 - val_precision_6: 0.6772 - val_recall_6: 0.4279\n",
      "Epoch 6/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.0764 - binary_accuracy: 0.9760 - precision_6: 0.9083 - recall_6: 0.7213 - val_loss: 0.2434 - val_binary_accuracy: 0.9478 - val_precision_6: 0.7009 - val_recall_6: 0.4080\n",
      "Epoch 7/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.0713 - binary_accuracy: 0.9775 - precision_6: 0.9134 - recall_6: 0.7395 - val_loss: 0.2492 - val_binary_accuracy: 0.9448 - val_precision_6: 0.6145 - val_recall_6: 0.5075\n",
      "Epoch 8/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.0677 - binary_accuracy: 0.9798 - precision_6: 0.9299 - recall_6: 0.7616 - val_loss: 0.2561 - val_binary_accuracy: 0.9482 - val_precision_6: 0.7182 - val_recall_6: 0.3930\n",
      "Epoch 9/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.0653 - binary_accuracy: 0.9809 - precision_6: 0.9284 - recall_6: 0.7798 - val_loss: 0.3586 - val_binary_accuracy: 0.9150 - val_precision_6: 0.4155 - val_recall_6: 0.6119\n",
      "Epoch 10/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.0626 - binary_accuracy: 0.9816 - precision_6: 0.9288 - recall_6: 0.7914 - val_loss: 0.3128 - val_binary_accuracy: 0.9441 - val_precision_6: 0.6233 - val_recall_6: 0.4527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c7c151940>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(x_train_v,y_train,epochs=10,validation_data=(x_test_v,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a495431",
   "metadata": {},
   "source": [
    "# CNN_N_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a48b5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_sentence(data):\n",
    "    max_len=0\n",
    "    for text in data:\n",
    "        text_len=len(text.split())\n",
    "        max_len=max(text_len,max_len)\n",
    "        \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7920458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_cnn=Data.iloc[0:10000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f67228e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>user father dysfunctional selfish drag kid dys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>user user thank lyft credit use cause offer wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>model love u take u time ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10566</th>\n",
       "      <td>0</td>\n",
       "      <td>hear ad radio blink summer tour promote dj pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10567</th>\n",
       "      <td>0</td>\n",
       "      <td>thankyoulordfohegiftoflife week everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10568</th>\n",
       "      <td>1</td>\n",
       "      <td>techjunkiejh alt right site plan fake black pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10569</th>\n",
       "      <td>0</td>\n",
       "      <td>tge dad father day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10570</th>\n",
       "      <td>0</td>\n",
       "      <td>make good point</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet\n",
       "0          0  user father dysfunctional selfish drag kid dys...\n",
       "1          0  user user thank lyft credit use cause offer wh...\n",
       "2          0                                     bihday majesty\n",
       "3          0                        model love u take u time ur\n",
       "4          0                      factsguide society motivation\n",
       "...      ...                                                ...\n",
       "10566      0  hear ad radio blink summer tour promote dj pla...\n",
       "10567      0           thankyoulordfohegiftoflife week everyone\n",
       "10568      1  techjunkiejh alt right site plan fake black pe...\n",
       "10569      0                                 tge dad father day\n",
       "10570      0                                    make good point\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0bd335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_input=get_longest_sentence(Data_cnn['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e91b8238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "843992b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad05ce5d682472a807612827e584d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 21 is out of bounds for axis 1 with size 21",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-5d4794d68725>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mdata_emb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 21 is out of bounds for axis 1 with size 21"
     ]
    }
   ],
   "source": [
    "data_emb = np.zeros((len(Data['tweet']), longest_input, 300))\n",
    "for i, text in enumerate(tqdm(nlp.pipe(Data['tweet']), total=len(Data['tweet']))):\n",
    "    for j, token in enumerate(text):\n",
    "        data_emb[i, j] = token.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ce135",
   "metadata": {},
   "source": [
    "#shape of data embedded (no of instances , max length of each sentence, 300 vct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5f34314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define the network\n",
    "inputs = tf.keras.layers.Input((longest_input, 300))\n",
    "reshaped = tf.keras.layers.Reshape((longest_input, 300, 1))(inputs)\n",
    "\n",
    "\n",
    "filters = [2, 3, 4]\n",
    "\n",
    "# define the conv net\n",
    "conv_1 = tf.keras.layers.Conv2D(100, (filters[0], 300), activation='relu')(reshaped)\n",
    "conv_2 = tf.keras.layers.Conv2D(100, (filters[1], 300), activation='relu')(reshaped)\n",
    "conv_3 = tf.keras.layers.Conv2D(100, (filters[2], 300), activation='relu')(reshaped)\n",
    "\n",
    "# define max-pooling\n",
    "pool_1 = tf.keras.layers.MaxPooling2D((longest_input - filters[0] + 1, 1), strides=(1,1))(conv_1)\n",
    "pool_2 = tf.keras.layers.MaxPooling2D((longest_input - filters[1] + 1, 1), strides=(1,1))(conv_2)\n",
    "pool_3 = tf.keras.layers.MaxPooling2D((longest_input - filters[2] + 1, 1), strides=(1,1))(conv_3)\n",
    "\n",
    "# concatenate the convs\n",
    "merged_tensor = tf.keras.layers.concatenate([pool_1, pool_2, pool_3], axis=1)\n",
    "\n",
    "# now flatten them and add a dense layer\n",
    "flatten = tf.keras.layers.Flatten()(merged_tensor)\n",
    "\n",
    "# add a dense layer\n",
    "clf = tf.keras.layers.Dense(100, activation='relu')(flatten)\n",
    "\n",
    "# add final output\n",
    "clf = tf.keras.layers.Dense(1, activation='sigmoid')(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "64e8fdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 37, 300)]    0           []                               \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 37, 300, 1)   0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 36, 1, 100)   60100       ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 35, 1, 100)   90100       ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 34, 1, 100)   120100      ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_3[0][0]',        \n",
      "                                                                  'max_pooling2d_4[0][0]',        \n",
      "                                                                  'max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 300)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 100)          30100       ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 1)            101         ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 300,501\n",
      "Trainable params: 300,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Model(inputs, clf)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f7a8a31",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.95 GiB for an array with shape (23624, 37, 300) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-0e5004ffa3ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2441\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2443\u001b[1;33m     return list(\n\u001b[0m\u001b[0;32m   2444\u001b[0m         chain.from_iterable(\n\u001b[0;32m   2445\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2443\u001b[0m     return list(\n\u001b[0;32m   2444\u001b[0m         chain.from_iterable(\n\u001b[1;32m-> 2445\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2446\u001b[0m         )\n\u001b[0;32m   2447\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.95 GiB for an array with shape (23624, 37, 300) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_emb, Data['label'], test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10,validation_data=(x_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
