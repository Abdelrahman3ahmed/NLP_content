{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-En1AU0EaP8P"
   },
   "source": [
    "**The same task as Lab 1 except using `Word Embeddings OR RNNs`**\n",
    "\n",
    "**Watch out!<br>\n",
    "Preproceesing here is a a bit different<br>\n",
    "Just take the cleaned data from the previous lab and cont...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ1Zm0iaaP8c"
   },
   "source": [
    "**Dataset**\n",
    "labeled datasset collected from twitter\n",
    "\n",
    "**Objective**\n",
    "classify tweets containing hate speech from other tweets.\n",
    "0 -> no hate speech\n",
    "1 -> contains hate speech\n",
    "\n",
    "**Total Estimated Time = 60 Mins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SneUFL7eaP8e"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z83UP2aVaP8f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "import contractions# load spacy model, can be \"en_core_web_sm\" as well\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAuIE393aP8h"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gcOjmxhcaP8i"
   },
   "outputs": [],
   "source": [
    "Data_set = pd.read_csv(\"dataset.csv\" , index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ArmLJHDaP8j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "660dB91maP8y"
   },
   "source": [
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UsmeTnKcaP8z"
   },
   "outputs": [],
   "source": [
    "duplicated_df = Data_set[Data_set[[\"tweet\" ,\"label\"]].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "23I-zEefaP8z"
   },
   "outputs": [],
   "source": [
    "Data_set = Data_set.drop(duplicated_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label  \\\n",
       "id          \n",
       "1       0   \n",
       "2       0   \n",
       "3       0   \n",
       "4       0   \n",
       "5       0   \n",
       "\n",
       "                                                                                                                         tweet  \n",
       "id                                                                                                                              \n",
       "1                        @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
       "2   @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "3                                                                                                          bihday your majesty  \n",
       "4                                       #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦    \n",
       "5                                                                                       factsguide: society now    #motivation  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    return \" \".join(re.sub(\"([\\@|\\#][A-Za-z0-9]+)|([^0-9A-Za-z \\t]) | r's+[a-zA-Z]s+' \", \" \",tweet.lower()).split())\n",
    "\n",
    "def remove_single_char(text):\n",
    "    return re.sub(r's+[a-zA-Z]s+', ' ', text)\n",
    "\n",
    "def text_preprocessing(text): \n",
    "    \n",
    "    text = process_tweet(text)\n",
    "    \n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    text = text.strip()\n",
    "    text =  \" \".join(text.split())\n",
    "\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "\n",
    "    doc = nlp(text) \n",
    "    clean_text = []\n",
    "    \n",
    "    for token in doc:\n",
    "        flag = True\n",
    "        edit = token.text\n",
    "        \n",
    "        if token.is_stop and token.pos_ != 'NUM': \n",
    "            flag = False\n",
    "        if token.pos_ == 'PUNCT' and flag == True: \n",
    "            flag = False\n",
    "        if token.pos_ == 'SYM' and flag == True: \n",
    "            flag = False\n",
    "        if (token.pos_ == 'NUM' or token.text.isnumeric()) \\\n",
    "        and flag == True:\n",
    "            flag = False\n",
    "        if token.pos_ == 'NUM' and flag == True:\n",
    "            edit = w2n.word_to_num(token.text)\n",
    "        elif token.lemma_ != \"-PRON-\" and flag == True:\n",
    "            edit = token.lemma_\n",
    "        if edit != \"\" and flag == True:\n",
    "            clean_text.append(edit) \n",
    "        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdelrahman\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Data_set[\"tweet\"] = Data_set[\"tweet\"].apply(lambda x:' '.join(text_preprocessing(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data = ' '.join(text_preprocessing(Data_set[\"tweet\"].iloc[9]))\n",
    "# remove_single_char(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>father dysfunctional selfish drag kid dysfunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>thank credit use offer wheelchair van pdx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>love time urd+- dddd d|d|d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide society</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              tweet\n",
       "id                                                          \n",
       "1       0  father dysfunctional selfish drag kid dysfunction\n",
       "2       0          thank credit use offer wheelchair van pdx\n",
       "3       0                                     bihday majesty\n",
       "4       0                         love time urd+- dddd d|d|d\n",
       "5       0                                 factsguide society"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZW7KFRD0aP81"
   },
   "source": [
    "### Modelling\n",
    "**Use any Advanced technique such as: word2vec, glove, RNNs ... etc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xHyPTsDRaP81"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer(num_words=15000 ,oov_token='UNK')\n",
    "tok.fit_on_texts(Data_set[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Hqfk4gi1aP83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "19853\n"
     ]
    }
   ],
   "source": [
    "print(tok.word_index['UNK'])\n",
    "print(len(tok.word_index))\n",
    "length_of_word_index = len(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target=Data_set[\"label\"]\n",
    "train_words, validation_words ,train_target ,validation_target = train_test_split(Data_set['tweet'],target,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_binary = tok.texts_to_matrix(train_words, mode='binary')\n",
    "validation_words_binary = tok.texts_to_matrix(validation_words, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23624,) (23624,)\n",
      "(5906,) (5906,)\n"
     ]
    }
   ],
   "source": [
    "print(train_words.shape , train_target.shape)\n",
    "print(validation_words.shape , validation_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers,models\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(layers.Dense(128,activation='relu',input_shape=(15000,)))\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(16,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdelrahman\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "739/739 [==============================] - 25s 32ms/step - loss: 0.6223 - binary_accuracy: 0.9229 - val_loss: 0.5348 - val_binary_accuracy: 0.9368\n",
      "Epoch 2/10\n",
      "739/739 [==============================] - 21s 28ms/step - loss: 0.4208 - binary_accuracy: 0.9306 - val_loss: 0.3086 - val_binary_accuracy: 0.9368\n",
      "Epoch 3/10\n",
      "739/739 [==============================] - 21s 28ms/step - loss: 0.2596 - binary_accuracy: 0.9306 - val_loss: 0.2272 - val_binary_accuracy: 0.9368\n",
      "Epoch 4/10\n",
      "739/739 [==============================] - 21s 29ms/step - loss: 0.2312 - binary_accuracy: 0.9306 - val_loss: 0.2210 - val_binary_accuracy: 0.9368\n",
      "Epoch 5/10\n",
      "739/739 [==============================] - 21s 29ms/step - loss: 0.2257 - binary_accuracy: 0.9306 - val_loss: 0.2182 - val_binary_accuracy: 0.9368\n",
      "Epoch 6/10\n",
      "739/739 [==============================] - 21s 28ms/step - loss: 0.2216 - binary_accuracy: 0.9306 - val_loss: 0.2161 - val_binary_accuracy: 0.9368\n",
      "Epoch 7/10\n",
      "739/739 [==============================] - 20s 27ms/step - loss: 0.2182 - binary_accuracy: 0.9306 - val_loss: 0.2146 - val_binary_accuracy: 0.9368\n",
      "Epoch 8/10\n",
      "739/739 [==============================] - 21s 28ms/step - loss: 0.2155 - binary_accuracy: 0.9306 - val_loss: 0.2135 - val_binary_accuracy: 0.9368\n",
      "Epoch 9/10\n",
      "739/739 [==============================] - 21s 29ms/step - loss: 0.2132 - binary_accuracy: 0.9306 - val_loss: 0.2129 - val_binary_accuracy: 0.9368\n",
      "Epoch 10/10\n",
      "739/739 [==============================] - 20s 28ms/step - loss: 0.2115 - binary_accuracy: 0.9306 - val_loss: 0.2124 - val_binary_accuracy: 0.9368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x190e20b4d00>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow import keras\n",
    "model.compile(optimizer= RMSprop(lr=2e-5),\n",
    "              loss= \"binary_crossentropy\",\n",
    "              metrics= [\"binary_accuracy\"])\n",
    "\n",
    "model.fit(train_words_binary,train_target,epochs=10,validation_data=(validation_words_binary,validation_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZVQRHjzaP83"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFmGwnbQaP84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b6NUw1UaP84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5buE2jA9aP8_"
   },
   "source": [
    "#### Done!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 2 - Text Classification using RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
