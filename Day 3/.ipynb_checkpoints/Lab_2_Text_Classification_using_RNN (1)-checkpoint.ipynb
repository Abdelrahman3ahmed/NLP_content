{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-En1AU0EaP8P"
   },
   "source": [
    "**The same task as Lab 1 except using `Word Embeddings OR RNNs`**\n",
    "\n",
    "**Watch out!<br>\n",
    "Preproceesing here is a a bit different<br>\n",
    "Just take the cleaned data from the previous lab and cont...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ1Zm0iaaP8c"
   },
   "source": [
    "**Dataset**\n",
    "labeled datasset collected from twitter\n",
    "\n",
    "**Objective**\n",
    "classify tweets containing hate speech from other tweets.\n",
    "0 -> no hate speech\n",
    "1 -> contains hate speech\n",
    "\n",
    "**Total Estimated Time = 60 Mins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SneUFL7eaP8e"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z83UP2aVaP8f"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode\n",
    "import contractions\n",
    "from word2number import w2n\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAuIE393aP8h"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gcOjmxhcaP8i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         @user when a father is dysfunctional and is s...\n",
       "1        @user @user thanks for #lyft credit i can't us...\n",
       "2                                      bihday your majesty\n",
       "3        #model   i love u take with u all the time in ...\n",
       "4                   factsguide: society now    #motivation\n",
       "                               ...                        \n",
       "31957    ate @user isz that youuu?ðððððð...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31960    @user #sikh #temple vandalised in in #calgary,...\n",
       "31961                     thank you @user for you follow  \n",
       "Name: tweet, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('dataset.csv')\n",
    "tweets['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "660dB91maP8y"
   },
   "source": [
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UsmeTnKcaP8z"
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    return \" \".join(re.sub(\"([\\@][A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",tweet.lower()).split())\n",
    " \n",
    "def text_preprocessing(text):\n",
    "    text = process_tweet(text)\n",
    "\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    " \n",
    "    text = text.strip()\n",
    "    text =  \" \".join(text.split())\n",
    " \n",
    "    text = unidecode.unidecode(text)\n",
    " \n",
    "    text = contractions.fix(text)\n",
    " \n",
    "    text = text.lower()\n",
    " \n",
    "    doc = nlp(text) \n",
    "    clean_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        flag = True\n",
    "        edit = token.text\n",
    "\n",
    "        if token.is_stop and token.pos_ != 'NUM': \n",
    "            flag = False\n",
    "#         if token.pos_ == 'PUNCT' and flag == True: \n",
    "#             flag = False\n",
    "        if token.pos_ == 'SYM' and flag == True: \n",
    "            flag = False\n",
    "        if (token.pos_ == 'NUM' or token.text.isnumeric()) \\\n",
    "        and flag == True:\n",
    "            flag = False\n",
    "        if token.pos_ == 'NUM' and flag == True:\n",
    "            edit = w2n.word_to_num(token.text)\n",
    "        elif token.lemma_ != \"-PRON-\" and flag == True:\n",
    "            edit = token.lemma_\n",
    "        if edit != \"\" and flag == True:\n",
    "            clean_text.append(edit)        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23I-zEefaP8z"
   },
   "outputs": [],
   "source": [
    "tweets['tweet'] = tweets['tweet'].apply(lambda x : \" \".join(text_preprocessing(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets['tweet'] = tweets['tweet'].apply(lambda x : \" \".join(process_tweet(x)))\n",
    "tweets['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZW7KFRD0aP81"
   },
   "source": [
    "### Modelling\n",
    "**Use any Advanced technique such as: word2vec, glove, RNNs ... etc**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHyPTsDRaP81"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(tweets['tweet'],tweets['label'],\n",
    "                                                   test_size=0.2, random_state=42,\n",
    "                                                   stratify=tweets['label'])\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hqfk4gi1aP83"
   },
   "outputs": [],
   "source": [
    "max_sequence = 0\n",
    "\n",
    "for sent in X_train:\n",
    "    max_sequence = max(len(sent),max_sequence)\n",
    "\n",
    "print(max_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vectorize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer(num_words=100 ,oov_token='UNK')\n",
    "tok.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tok.word_index['UNK'])\n",
    "print(len(tok.word_index))\n",
    "length_of_word_index = len(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_v = np.zeros((len(X_train), 300))\n",
    "x_test_v = np.zeros((len(X_test), 300))\n",
    "\n",
    "for i, doc in tqdm(enumerate(nlp.pipe(X_train)), total=len(X_train)):\n",
    "    x_train_v[i, :] = doc.vector\n",
    "\n",
    "for i, doc in tqdm(enumerate(nlp.pipe(X_test)), total=len(X_test)):\n",
    "    x_test_v[i, :] = doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 100\n",
    "\n",
    "x_train_padded = np.zeros((x_train_v.shape[0], max_sequence_len))\n",
    "\n",
    "for i, sent in enumerate(x_train_v):\n",
    "    x_train_padded[i, :len(sent)] = sent[:max_sequence_len]\n",
    "x_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_padded = np.zeros((x_test_v.shape[0], max_sequence_len))\n",
    "\n",
    "for i, sent in enumerate(x_test_v):\n",
    "    x_test_padded[i, :len(sent)] = sent[:max_sequence_len]\n",
    "x_test_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tweets)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([    \n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 64),\n",
    "    tf.keras.layers.SimpleRNN(64),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_padded, y_train, epochs=5, batch_size=128,\n",
    "                    validation_data=(x_test_padded, y_test), \n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test_padded, y_test)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZVQRHjzaP83"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFmGwnbQaP84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b6NUw1UaP84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5buE2jA9aP8_"
   },
   "source": [
    "#### Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 2 - Text Classification using RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
