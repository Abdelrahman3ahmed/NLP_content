{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-En1AU0EaP8P"
   },
   "source": [
    "**The same task as Lab 1 except using `Word Embeddings OR RNNs`**\n",
    "\n",
    "**Watch out!<br>\n",
    "Preproceesing here is a a bit different<br>\n",
    "Just take the cleaned data from the previous lab and cont...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ1Zm0iaaP8c"
   },
   "source": [
    "**Dataset**\n",
    "labeled datasset collected from twitter\n",
    "\n",
    "**Objective**\n",
    "classify tweets containing hate speech from other tweets.\n",
    "0 -> no hate speech\n",
    "1 -> contains hate speech\n",
    "\n",
    "**Total Estimated Time = 60 Mins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SneUFL7eaP8e"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "Z83UP2aVaP8f"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode\n",
    "import contractions\n",
    "from word2number import w2n\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAuIE393aP8h"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "gcOjmxhcaP8i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         @user when a father is dysfunctional and is s...\n",
       "1        @user @user thanks for #lyft credit i can't us...\n",
       "2                                      bihday your majesty\n",
       "3        #model   i love u take with u all the time in ...\n",
       "4                   factsguide: society now    #motivation\n",
       "                               ...                        \n",
       "31957    ate @user isz that youuu?ðððððð...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31960    @user #sikh #temple vandalised in in #calgary,...\n",
       "31961                     thank you @user for you follow  \n",
       "Name: tweet, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('dataset.csv')\n",
    "tweets['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "660dB91maP8y"
   },
   "source": [
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "UsmeTnKcaP8z"
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    return \" \".join(re.sub(\"([\\@][A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",tweet.lower()).split())\n",
    " \n",
    "def text_preprocessing(text):\n",
    "    text = process_tweet(text)\n",
    "\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    " \n",
    "    text = text.strip()\n",
    "    text =  \" \".join(text.split())\n",
    " \n",
    "    text = unidecode.unidecode(text)\n",
    " \n",
    "    text = contractions.fix(text)\n",
    " \n",
    "    text = text.lower()\n",
    " \n",
    "    doc = nlp(text) \n",
    "    clean_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        flag = True\n",
    "        edit = token.text\n",
    "\n",
    "        if token.is_stop and token.pos_ != 'NUM': \n",
    "            flag = False\n",
    "#         if token.pos_ == 'PUNCT' and flag == True: \n",
    "#             flag = False\n",
    "        if token.pos_ == 'SYM' and flag == True: \n",
    "            flag = False\n",
    "        if (token.pos_ == 'NUM' or token.text.isnumeric()) \\\n",
    "        and flag == True:\n",
    "            flag = False\n",
    "        if token.pos_ == 'NUM' and flag == True:\n",
    "            edit = w2n.word_to_num(token.text)\n",
    "        elif token.lemma_ != \"-PRON-\" and flag == True:\n",
    "            edit = token.lemma_\n",
    "        if edit != \"\" and flag == True:\n",
    "            clean_text.append(edit)        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "23I-zEefaP8z"
   },
   "outputs": [],
   "source": [
    "tweets['tweet'] = tweets['tweet'].apply(lambda x : \" \".join(text_preprocessing(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        father dysfunctional selfish drag kid dysfunct...\n",
       "1        thank lyft credit t use don t offer wheelchair...\n",
       "2                                           bihday majesty\n",
       "3                                          model love time\n",
       "4                            factsguide society motivation\n",
       "                               ...                        \n",
       "31957                                        eat isz youuu\n",
       "31958    nina turner airwave try wrap mantle genuine he...\n",
       "31959          listen sad song monday morning otw work sad\n",
       "31960        sikh temple vandalise calgary wso condemn act\n",
       "31961                                         thank follow\n",
       "Name: tweet, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets['tweet'] = tweets['tweet'].apply(lambda x : \" \".join(process_tweet(x)))\n",
    "tweets['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZW7KFRD0aP81"
   },
   "source": [
    "### Modelling\n",
    "**Use any Advanced technique such as: word2vec, glove, RNNs ... etc**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "xHyPTsDRaP81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25569\n",
      "6393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(tweets['tweet'],tweets['label'],\n",
    "                                                   test_size=0.2, random_state=42,\n",
    "                                                   stratify=tweets['label'])\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "Hqfk4gi1aP83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    }
   ],
   "source": [
    "max_sequence = 0\n",
    "\n",
    "for sent in X_train:\n",
    "    max_sequence = max(len(sent),max_sequence)\n",
    "\n",
    "print(max_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vectorize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer(num_words=100 ,oov_token='UNK')\n",
    "tok.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "29623\n"
     ]
    }
   ],
   "source": [
    "print(tok.word_index['UNK'])\n",
    "print(len(tok.word_index))\n",
    "length_of_word_index = len(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1e88fb2f324e5eada2790c68913186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25569 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6bd76de85449e6a0471c65bff9d52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_v = np.zeros((len(X_train), 300))\n",
    "x_test_v = np.zeros((len(X_test), 300))\n",
    "\n",
    "for i, doc in tqdm(enumerate(nlp.pipe(X_train)), total=len(X_train)):\n",
    "    x_train_v[i, :] = doc.vector\n",
    "\n",
    "for i, doc in tqdm(enumerate(nlp.pipe(X_test)), total=len(X_test)):\n",
    "    x_test_v[i, :] = doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25569, 100)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len = 100\n",
    "\n",
    "x_train_padded = np.zeros((x_train_v.shape[0], max_sequence_len))\n",
    "\n",
    "for i, sent in enumerate(x_train_v):\n",
    "    x_train_padded[i, :len(sent)] = sent[:max_sequence_len]\n",
    "x_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6393, 100)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_padded = np.zeros((x_test_v.shape[0], max_sequence_len))\n",
    "\n",
    "for i, sent in enumerate(x_test_v):\n",
    "    x_test_padded[i, :len(sent)] = sent[:max_sequence_len]\n",
    "x_test_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31962"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tweets)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([    \n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 64),\n",
    "    tf.keras.layers.SimpleRNN(64),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 64)          2045568   \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,058,049\n",
      "Trainable params: 2,058,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25569, 100)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_padded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_train_padded_ \u001b[38;5;241m=\u001b[39m \u001b[43mx_train_padded\u001b[49m[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m      2\u001b[0m y_train_ \u001b[38;5;241m=\u001b[39m y_train[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m100\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train_padded' is not defined"
     ]
    }
   ],
   "source": [
    "x_train_padded_ = x_train_padded[0:100]\n",
    "y_train_ = y_train[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_padded_, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kljewlkcewvwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test_padded, y_test)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZVQRHjzaP83"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFmGwnbQaP84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b6NUw1UaP84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5buE2jA9aP8_"
   },
   "source": [
    "#### Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 2 - Text Classification using RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
